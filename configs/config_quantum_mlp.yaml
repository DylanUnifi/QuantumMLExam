experiment_name: "quantum_mlp_mnist"

dataset: "MNIST"
selected_classes: [0, 1]

batch_size: 64
epochs: 5
learning_rate: 0.001
kfold: 5
early_stopping: 7
scheduler: "step"  # options: None, step, cosine, etc.

model:
  input_size: 784  # 28x28 flattened MNIST
  hidden_sizes: [256, 128, 64]  # optional if tu veux les customiser

checkpoint:
  save_dir: "checkpoints"
  subdir: "quantum_mlp"
